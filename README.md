<div align="center">
    <h1>Awesome Spatial Intelligence in VLM</h1>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"/></a>
    <img src=https://img.shields.io/github/stars/yyyybq/Awesome-Spatial-Reasoning.svg?style=social >
</div>


This is a carefully curated and maintained list of resources, bringing together key methods, datasets and benchmarks in the field of Spatial Intelligence in VLM .

Spatial reasoning is a core capability for achieving Artificial General Intelligence (AGI) and Physical Intelligence. It requires models to not only "see" objects but to also understand their positions, relationships, geometry, and dynamic changes within the 3D world.

With the development of multi-modal models, evaluating and enhancing their spatial intelligence has become a key research frontier. This list aims to provide researchers and engineers with a quick index to track the latest advancements in the field.

We welcome contributions of excellent resources you find via Pull Request!




## Table of Contents
- [Awesome Spatial Intelligence in VLM](#Awesome Spatial Intelligence in VLM)
  <!-- - [Overview](#Overview-of-Embodied-Multimodal-LLMs) -->
  - [Methods](#Methods)
  - [Datasets & Benchmark](#Datasets-&-Benchmark)
  - [Applications](#Applications)



<!-- ## Overview



Spatial thinking is the foundation of abstract thought. Human uses spatial thinking everywhere (math, 
social network, flow chart, mind map). It's also important for LLM or VLM to build spatial cognition ability to perceive and interact with surroundings. We evaluate spatial ability based on text, single or multi-view images, 3d image or video as input. We enhance spatial ability with visual prompt, text input, image input or image input with 3D. -->




## Methods
### Visual based
| Title                                                        |                        Introduction                         |    Date    |                             Code                             |
| :----------------------------------------------------------- | :---------------------------------------------------------: | :--------: | :----------------------------------------------------------: 
| <br/>[Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation](https://arxiv.org/abs/2510.08673) | <img width="700" alt="image" src="imgs/Puffin.png"> | 2025-10 | [Github](https://kangliao929.github.io/projects/puffin/) |
| <br/>[Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views](https://arxiv.org/html/2510.18632) | <img width="700" alt="image" src="imgs/3DThinker.png"> | 2025-10 | [Github](https://github.com/zhangquanchen/3DThinker) |
| <br/>[Euclid’s Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks](https://arxiv.org/abs/2509.24473) | <img width="700" alt="image" src="imgs/Euclid.png"> | 2025-10 | [Github](https://github.com/LiamLian0727/Euclids_Gift) |
| <br/>[SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.08531) | <img width="700" alt="image" src="imgs/SpatialLadder.png"> | 2025-08 | [Github](https://zju-real.github.io/SpatialLadder) |
| <br/>[SpaceVista: All-Scale Visual Spatial Reasoning from mm to km](https://arxiv.org/abs/2510.09606) | <img width="700" alt="image" src="imgs/SpatialVista.png"> | 2025-08 | [Github](https://peiwensun2000.github.io/mm2km/) |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model](https://arxiv.org/pdf/2509.16087) | <img width="700" alt="image" src="imgs/seetrek.png"> | 2025-09 | -|
| <br/>[3D Aware Region Prompted Vision Language Model](https://arxiv.org/abs/2509.13317) | <img width="700" alt="image" src="imgs/SR3D.png"> | 2025-09 | [Github](https://www.anjiecheng.me/sr3d) |
| <br/>[UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952) | <img width="700" alt="image" src="imgs/UniUGG.png"> | 2025-08 | [Github](https://fudan-zvg.github.io/UniUGG) |
| <br/>[Enhancing Spatial Reasoning through Visual and Textual Thinking](https://arxiv.org/abs/2507.20529) | <img width="700" alt="image" src="imgs/SpatialVTS.png"> | 2025-07 | - |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[MindJourney: Test-Time Scaling with World Models for Spatial Reasoning](https://www.arxiv.org/abs/2507.12508) | <img width="700" alt="image" src="imgs/MindJourney.png"> | 2025-07 | [Github](https://umass-embodied-agi.github.io/MindJourney/) |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models](https://arxiv.org/pdf/2506.04220) | <img width="700" alt="image" src="imgs/Struct2D.png"> | 2025-06 | [Github](https://github.com/neu-vi/struct2d) |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656) | <img width="700" alt="image" src="imgs/SpatialReasoner-R1.png"> | 2025-06 | [Github](https://plan-lab.github.io/projects/spatialreasoner/) |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491) | <img width="700" alt="image" src="imgs/SpatialLM.png"> | 2025-06 | [Github](https://manycore-research.github.io/SpatialLM/) |
| <br/>[Spatial Understanding from Videos: Structured Prompts Meet Simulation Data](https://arxiv.org/abs/2506.03642) | <img width="700" alt="image" src="imgs/SpatialMind.png"> | 2025-06 | [Github](https://github.com/Hyu-Zhang/SpatialMind) |
| <br/>[Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/pdf/2506.09965) | <img width="700" alt="image" src="imgs/ViLaSR.png"> | 2025-06 | [Github](https://github.com/AntResearchNLP/ViLaSR) |
| <br/>[Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces](https://arxiv.org/pdf/2506.00123v1) | <img width="700" alt="image" src="imgs/Vebrain.png"> | 2025-05 | [Github](https://internvl.github.io/blog/2025-05-26-VeBrain/) |
| [![Publish](https://img.shields.io/badge/Conference-Neurips'25-green)]()<br/>[RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/pdf/2506.04308) | <img width="700" alt="image" src="imgs/RoboRefer.png"> | 2025-05 | [Github](https://github.com/Zhoues/RoboRefer) |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors](https://arxiv.org/pdf/2505.24625) |   <img width="700" alt="image" src="imgs/VGLLM.png">   | 2025-05 |    [Github](https://lavi-lab.github.io/VG-LLM)   |
| [![Publish](https://img.shields.io/badge/Conference-CVPR'25-blue)]()<br/>[SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models](https://arxiv.org/abs/2505.00788) |   <img width="700" alt="image" src="imgs/SpatialLLM.png">   | 2025-05 |    [Github](https://3d-spatial-reasoning.github.io/spatial-llm/)   |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[ Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/pdf/2505.23747) |   <img width="700" alt="image" src="imgs/Spatial-MLLM.png">   | 2025-05 |    [Github](https://diankun-wu.github.io/Spatial-MLLM/)   |
| <br/>[STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs](https://arxiv.org/abs/2505.15804) |   <img width="700" alt="image" src="imgs/STAR-R1.png">   | 2025-05 |    [Github](https://github.com/zongzhao23/STAR-R1)   |
| <br/>[VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/pdf/2505.20279) |   <img width="700" alt="image" src="imgs/VLM-3R.png">   | 2025-05 |    [Github](https://github.com/VITA-Group/VLM-3R)   |
| <br/>[LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding](https://arxiv.org/abs/2505.12253) |   <img width="700" alt="image" src="imgs/LLaVA-4D.png">   | 2025-05 |        -    |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning](https://arxiv.org/abs/2505.12448) |   <img width="700" alt="image" src="imgs/SSR.png">   | 2025-05 |    [Github](https://yliu-cs.github.io/SSR)        |
| [![Publish](https://img.shields.io/badge/Conference-ICCV'25-blue)]()<br/>[Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation](https://arxiv.org/pdf/2504.17207) |   <img width="700" alt="image" src="imgs/APC.png">   | 2025-04 |     [Github](https://apc-vlm.github.io/)     |
| <br/>[SpaceR: Reinforcing MLLMs in Video Spatial Reasoning](https://arxiv.org/abs/2504.01805) |   <img width="700" alt="image" src="imgs/SpaceR.png">   | 2025-04 |          [Github](https://github.com/OuyangKun10/SpaceR)         |
| <br/>[Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning](https://arxiv.org/abs/2504.12680) |   <img width="700" alt="image" src="imgs/Embodied-R.png">   | 2025-04 |          [Github](https://github.com/EmbodiedCity/Embodied-R.code)   |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning](https://arxiv.org/abs/2504.20024) |   <img width="700" alt="image" src="imgs/SpatialReasoner.png">   | 2025-04 |  [Github](https://spatial-reasoner.github.io/) |
| [![Publish](https://img.shields.io/badge/Conference-ICCV'25-blue)]()<br/>[ROSS3D: Reconstructive Visual Instruction Tuning with 3D-Awareness](https://arxiv.org/pdf/2504.01901) |   <img width="700" alt="image" src="imgs/Ross3D.png">   | 2025-04 |      [Github](https://github.com/haochen-wang409/ross3d)   |
|  [![Publish](https://img.shields.io/badge/Conference-ACL'25-blue)]()<br/>[SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data](https://arxiv.org/abs/2504.20648) |   <img width="700" alt="image" src="imgs/SpaRE.png">   | 2025-04 |      -     |
| <br/>[Visual Agentic AI for Spatial Reasoning with a Dynamic API](https://arxiv.org/pdf/2502.06787) |   <img width="700" alt="image" src="imgs/VADAR.png">   | 2025-02 |         [Github](https://glab-caltech.github.io/vadar/)         |
| <br/>[SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning](https://arxiv.org/abs/2501.10074) |   <img width="700" alt="image" src="imgs/SpatialCoT.jpg">   | 2025-01 |         [Github](https://spatialcot.github.io/)         |
| <br/>[COARSE CORRESPONDENCES Boost Spatial-Temporal Reasoning in Multimodal Language Model](https://arxiv.org/pdf/2408.00754) |   <img width="700" alt="image" src="imgs/correspondence.png">   | 2024-08 |         [Github](https://coarse-correspondence.github.io/)         |
| [![Star](https://img.shields.io/github/stars/AnjieCheng/SpatialRGPT.svg?style=social&label=Star)]() [![Publish](https://img.shields.io/badge/Conference-NIPS'24-blue)]()<br/>[SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models](https://arxiv.org/abs/2406.01584) |   <img width="700" alt="image" src="imgs/SpatialRGPT.png">   | 2024-06 |         [Github](https://github.com/AnjieCheng/SpatialRGPT)         |
[![Star](https://img.shields.io/github/stars/BAAI-DCAI/SpatialBot.svg?style=social&label=Star)]() [![Publish](https://img.shields.io/badge/Conference-ICRA'24-blue)]()<br/>[SpatialBot: Precise Spatial Understanding with Vision Language Models](https://arxiv.org/abs/2406.13642) |   <img width="700" alt="image" src="imgs/SpatialBot.png">   | 2024-06 |       [Github](https://github.com/BAAI-DCAI/SpatialBot)     |
|  [![Publish](https://img.shields.io/badge/Conference-CVPR'24-blue)]()<br/>[Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs](https://arxiv.org/abs/2404.07449) |   <img width="700" alt="image" src="imgs/local.png">   | 2024-04 |         [Github](https://github.com/kahnchana/locvlm)         |
|  <br/>[SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors](https://arxiv.org/pdf/2403.13438) |   <img width="700" alt="image" src="imgs/SpatialPIN.png">   | 2024-03 | - |
|  [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]()<br/>[Can Transformers Capture Spatial Relations between Objects?](https://arxiv.org/abs/2403.00729) |   <img width="700" alt="image" src="imgs/SpatialSense.png">   | 2024-03 |  [Github](https://github.com/AlvinWen428/spatial-relation-benchmark) |
| [![Star](https://img.shields.io/github/stars/remyxai/VQASynth.svg?style=social&label=Star)]() [![Publish](https://img.shields.io/badge/Conference-CVPR'24-green)]()<br/>[SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities](https://arxiv.org/abs/2401.12168) |   <img width="700" alt="image" src="imgs/SpatialVLM.jpg">  | 2024-01 | [Github](https://github.com/remyxai/VQASynth)|
|  <br/>[Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis](https://arxiv.org/abs/2401.17862) |   <img width="700" alt="image" src="imgs/ProximityQA.png">   | 2024-01 |         [Github](https://github.com/NorthSummer/ProximityQA)         |
| <br/>[3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V](https://arxiv.org/abs//2312.09738) |   <img width="700" alt="image" src="imgs/Axies.png">   | 2023-12 |    -   |


### Text based
| Title                                                        |                        Introduction                         |    Date    |                             Code                             |
| :----------------------------------------------------------- | :---------------------------------------------------------: | :--------: | :----------------------------------------------------------: |
| <br/>[Imagine while Reasoning in Space: Multimodal Visualization-of-Thought](https://arxiv.org/abs/2501.07542) |   <img width="700" alt="image" src="imgs/MVoT.png">   | 2025-01 |   -       |
| <br/>[Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs](https://arxiv.org/abs/2411.18564) |   <img width="700" alt="image" src="imgs/ASP.png">   | 2024-11 |       -       |
|  [![Publish](https://img.shields.io/badge/Conference-NIPS'24-blue)]()<br/>[Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning](https://arxiv.org/abs/2410.16162) |   <img width="700" alt="image" src="imgs/Sparkle.png">   | 2024-10 |     -     |
|  [![Publish](https://img.shields.io/badge/Conference-ACL'24-blue)]()<br/>[SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models](https://arxiv.org/abs/2406.04566) |   <img width="700" alt="image" src="imgs/SpaRP.png">   | 2024-06 |         [Github](https://github.com/UKPLab/acl2024-sparc-and-sparp)         |
|  [![Publish](https://img.shields.io/badge/Conference-EMNLP'24-blue)]()<br/>[Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models](https://arxiv.org/abs/2402.03877) |   <img width="700" alt="image" src="imgs/Geometric.png">   | 2024-02 |       -       |
|  [![Publish](https://img.shields.io/badge/Conference-AAAI'24-blue)]()<br/>[Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark](https://arxiv.org/abs/2401.03991) |   <img width="700" alt="image" src="imgs/StepGame.png">   | 2024-01 |      [Github](https://github.com/Fangjun-Li/SpatialLM-StepGame)    |


## Datasets & Benchmark
### Visual based
| Title                                                        |                         Introduction                         |    Date    |                           Code                           |
| :----------------------------------------------------------- | :----------------------------------------------------------: | :--------: | :------------------------------------------------------: |
|  <br/>[DSI-Bench: A Benchmark for Dynamic Spatial Intelligence](https://arxiv.org/abs/2510.18873) |   <img width="700" alt="image" src="imgs/DSI-Bench.png">   | 2025-10 |          [Github](https://dsibench.github.io/) |
|  <br/>[Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes](https://arxiv.org/pdf/2510.19400) |   <img width="700" alt="image" src="imgs/MV-RoboBench.png">   | 2025-10 |           - |
|  <br/>[NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173) |   <img width="700" alt="image" src="imgs/NavSpace.png">   | 2025-10 |        [Github](https://navspace.github.io/#NavSpace-Leaderboard)  |
|  <br/>[SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs](https://arxiv.org/abs/2509.25390) |   <img width="700" alt="image" src="imgs/SpinBench.png">   | 2025-09 |          [Github](https://spinbench25.github.io/) |
|  <br/>[Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes](https://arxiv.org/pdf/2509.06266) |   <img width="700" alt="image" src="imgs/Ego3D-Bench.png">   | 2025-09 |             [Github](https://vbdi.github.io/Ego3D-Bench-webpage/) |
|  <br/>[Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture](https://arxiv.org/abs/2509.02359) |   <img width="700" alt="image" src="imgs/MulSeT.png">   | 2025-09 |     [Github](https://github.com/WanyueZhang-ai/spatial-understanding)   |
|  <br/>[SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676) |   <img width="700" alt="image" src="imgs/SpatailVID.png">   | 2025-09 |     [Github](https://nju-3dv.github.io/projects/SpatialVID)   |
|  <br/>[11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/html/2508.20068v1) |   <img width="700" alt="image" src="imgs/11Plus.png">   | 2025-08 |   -  |
| [![Publish](https://img.shields.io/badge/Conference-ICCV'25-blue)]() <br/>[Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting](https://arxiv.org/pdf/2507.18678) |   <img width="700" alt="image" src="imgs/scalable.png">   | 2025-07 |     [Github](https://zhanggongjie.github.io/TowardsSSI-page/)   |
|  <br/>[Ascending the Infinite Ladder: Benchmarking Spatia](https://arxiv.org/pdf/2507.02978v1) |   <img width="700" alt="image" src="imgs/Inf-Bench.png"> | 2025-07 | -  |
|  <br/>[SpatialViz-Bench: An MLLM Benchmark for Spatial Visualization](https://web3.arxiv.org/pdf/2507.07610) |   <img width="700" alt="image" src="imgs/SpatialViz.png"> | 2025-07 | [Github](https://huggingface.co/datasets/PLM-Team/Spatial-Visualization-Benchmark)   |
|  <br/>[Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458) |   <img width="700" alt="image" src="imgs/mindcube.png">   | 2025-06 |             [Github](https://mind-cube.github.io/) |
|  <br/>[SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks](https://arxiv.org/pdf/2506.14512) |   <img width="700" alt="image" src="imgs/SIRI.png">   | 2025-06 |        - |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering](https://arxiv.org/abs/2506.23329) | <img width="700" alt="image" src="imgs/IR3D.png"> | 2025-06 | [Github](https://ir3d-bench.github.io/) |
| [![Publish](https://img.shields.io/badge/Conference-NeurIPS'25-green)]()<br/>[PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly](https://arxiv.org/abs/2506.08708) |   <img width="700" alt="image" src="imgs/PhyBlock.png">   | 2025-06 |             [Github](https://phyblock.github.io/) 
|  <br/>[Can Vision Language Models Infer Human Gaze Direction? A Controlled Study](https://arxiv.org/abs/2506.05412) |   <img width="700" alt="image" src="imgs/Gaze.png">   | 2025-06 |             [Github](https://grow-ai-like-a-child.github.io/gaze/) 
|  <br/>[SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence](https://arxiv.org/abs/2506.07966) |   <img width="700" alt="image" src="imgs/SpaCE-10.png">   | 2025-06 |             [Github](https://github.com/VisionXLab/SpaCE-10) 
|  <br/>[Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations](https://arxiv.org/pdf/2506.04633) |   <img width="700" alt="image" src="imgs/STARE.png">   | 2025-06 |             [Github](https://github.com/VisionXLab/SpaCE-10) 
|  <br/>[OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/pdf/2506.03135) |   <img width="700" alt="image" src="imgs/OmniSpatial.png">   | 2025-06 |             [Github](https://github.com/qizekun/OmniSpatial) 
|  <br/>[InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models](https://arxiv.org/pdf/2506.18385) |   <img width="700" alt="image" src="imgs/InternSpatial.png">   | 2025-06 |           -|
|  <br/>[MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/pdf/2505.23764) |   <img width="700" alt="image" src="imgs/MMSI.png">   | 2025-05 |             [Github](https://runsenxu.com/projects/MMSI_Bench) 
|  <br/>[Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/pdf/2505.17015) |   <img width="700" alt="image" src="imgs/Multi-SpatialMLLM.png">   | 2025-05 |             [Github](https://runsenxu.com/projects/Multi-SpatialMLLM)  
|  <br/>[SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding](https://arxiv.org/pdf/2505.17012) |   <img width="700" alt="image" src="imgs/SpatialScore.png">   | 2025-05 |  [Github](https://haoningwu3639.github.io/SpatialScore) |
|  <br/>[MIRAGE:A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence](https://arxiv.org/pdf/2505.10604) |   <img width="700" alt="image" src="imgs/Mirage.png">   | 2025-05 |  [Github](https://github.com/khazic/Mirage) |
|  <br/>[Can Multimodal Large Language Models Understand Spatial Relations](https://arxiv.org/pdf/2505.10604) |   <img width="700" alt="image" src="imgs/SpatialMQA.png">   | 2025-05 |  [Github](https://github.com/ziyan-xiaoyu/SpatialMQA.git) |
|  <br/>[Visuospatial Cognitive Assistant](https://arxiv.org/pdf/2505.12312) |   <img width="700" alt="image" src="imgs/ViCA.png">   | 2025-05 |             [Github](https://huggingface.co/nkkbr/ViCA) 
|  <br/>[Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?](https://arxiv.org/pdf/2505.19015) |   <img width="700" alt="image" src="imgs/OmniSR.png">   | 2025-05 | [Github](https://huggingface.co/datasets/UUUserna/OSR-Bench) 
|  <br/>[Vision language models have difficulty recognizing virtual objects](https://arxiv.org/abs/2505.10453) |   <img width="700" alt="image" src="imgs/virtual.png">   | 2025-05 | -
|  <br/>[ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/pdf/2505.21500) |   <img width="700" alt="image" src="imgs/ViewSpatial-Bench.png">   | 2025-05 | [Github](https://zju-real.github.io/ViewSpatial-Page) |
|  <br/>[Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames](https://arxiv.org/abs/2505.24257) |   <img width="700" alt="image" src="imgs/Disjoint-3DQA.png">   | 2025-05 | - |
|  [![Publish](https://img.shields.io/badge/Conference-ICCV'25-blue)]()<br/>[SITE: towards Spatial Intelligence Thorough Evaluation](https://arxiv.org/pdf/2505.05456) |   <img width="700" alt="image" src="imgs/SITE.png">   | 2025-05 |   [Github](https://wenqi-wang20.github.io/SITE-Bench.github.io/)  
|  <br/>[CameraBench: Towards Understanding Camera Motions in Any Video](http://arxiv.org/abs/2504.15376) |   <img width="700" alt="image" src="imgs/CameraBench.png">   | 2025-04 |  [Github](https://linzhiqiu.github.io/papers/camerabench/)  
|  <br/>[Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](http://arxiv.org/abs/2504.15280) |   <img width="700" alt="image" src="imgs/All-Angle.png">  | 2025-04 | [Github](https://danielchyeh.github.io/All-Angles-Bench/) 
|  <br/>[From Flatland to Space:Teaching Vision-Language Models to Perceive and Reason in 3D](https://arxiv.org/abs/2503.11094) |   <img width="700" alt="image" src="imgs/PAR.png">   | 2025-03 | [Github](https://fudan-zvg.github.io/spar) 
|  <br/>[MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLM](https://arxiv.org/abs/2503.13111) |   <img width="700" alt="image" src="imgs/CA-VQA.png">   | 2025-03 | -
|  <br/>[Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space](https://arxiv.org/abs/2503.11094) |   <img width="700" alt="image" src="imgs/Open3DVQA.png">   | 2025-03 |             [Github](https://github.com/WeichenZh/Open3DVQA)
|  [![Publish](https://img.shields.io/badge/Conference-ICCV'25-blue)]()<br/>[STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?](http://arxiv.org/abs/2503.23765) |   <img width="700" alt="image" src="imgs/STI-Bench.png">   | 2025-03 |             [Github](https://mint-sjtu.github.io/STI-Bench.io/)      
|  [![Publish](https://img.shields.io/badge/Conference-CVPR'25-green)]()<br/>[CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models](https://arxiv.org/abs/2503.14161) |   <img width="700" alt="image" src="imgs/CoSpace.png">   | 2025-03 |             [Github](https://github.com/THUNLP-MT/CoSpace/)   
|  <br/>[Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models](https://arxiv.org/pdf/2503.19707) |   <img width="700" alt="image" src="imgs/SRBench.png">   | 2025-03 |    [Github](https://github.com/stogiannidis/srbench)
|  <br/>[LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?](http://arxiv.org/abs/2503.19990) |   <img width="700" alt="image" src="imgs/LEGO.png">   | 2025-03 |            [Github](https://tangkexian.github.io/LEGO-Puzzles/)
|  [![Publish](https://img.shields.io/badge/Conference-CVPR'25-green)]()<br/>[Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models](https://arxiv.org/abs/2502.08636) |   <img width="700" alt="image" src="imgs/Spatial457.png">   | 2025-02 |             [Github](https://xingruiwang.github.io/projects/Spatial457/) 
|  <br/>[FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks](https://arxiv.org/abs/2502.17775) |   <img width="700" alt="image" src="imgs/FoR.png">   | 2025-02 |      - |
|  <br/>[iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs](https://arxiv.org/abs/2502.03214) |   <img width="700" alt="image" src="imgs/iVISPAR.png">   | 2025-02 | [Github](https://github.com/SharkyBamboozle/iVISPAR)  |
|  <br/>[Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics](http://arxiv.org/abs/2502.11859) |   <img width="700" alt="image" src="imgs/BSA.png">   | 2025-02 |    -          |
|  [![Publish](https://img.shields.io/badge/Conference-CoLM'25-blue)]()<br/>[SAT: Spatial Aptitude Training for Multimodal Language Models](https://arxiv.org/abs/2412.07755) |   <img width="700" alt="image" src="imgs/sat.png">   | 2024-12 |             [Github](https://arijitray1993.github.io/SAT/)          |
| [![Publish](https://img.shields.io/badge/Conference-ACL'25-blue)]()<br/>[SPHERE: A Hierarchical Evaluation on Spatial Perception and Reasoning for Vision-Language Models](https://arxiv.org/abs/2412.12693) |   <img width="700" alt="image" src="imgs/SPHERE.png">   | 2024-12 |             [Github](https://github.com/zwenyu/SPHERE-VLM)          |
|  [![Publish](https://img.shields.io/badge/Conference-ICCV'25-blue)]()<br/>[3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark](https://arxiv.org/abs/2412.07825) |   <img width="700" alt="image" src="imgs/3DSRBench.png">   | 2024-12 |             [Github](https://3dsrbench.github.io/)          |
|  [![Star](https://img.shields.io/github/stars/vision-x-nyu/thinking-in-space.svg?style=social&label=Star)]()[![Publish](https://img.shields.io/badge/Conference-CVPR'25-green)]()<br/>[Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces](https://arxiv.org/abs/2412.14171) |   <img width="700" alt="image" src="imgs/Think_in_space.png">   | 2024-12 |             [Github](https://github.com/vision-x-nyu/thinking-in-space)          |
|  [![Publish](https://img.shields.io/badge/Conference-CVPR'25-green)]()<br/>[RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics](https://arxiv.org/abs/2411.16537) |   <img width="700" alt="image" src="imgs/RoboSpatial.png">   | 2024-11 |         -   |
|  [![Publish](https://img.shields.io/badge/Conference-EMNLP'24-blue)]()<br/>[An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models](https://arxiv.org/abs/2411.06048) |   <img width="700" alt="image" src="imgs/SpatialMM.png">   | 2024-11 |  -     |
|  <br/>[Is ‘Right’ Right? Enhancing Object Orientation Understanding in Multimodal Language Models through Egocentric Instruction Tuning](https://arxiv.org/pdf/2411.16761v1) |   <img width="700" alt="image" src="imgs/EgoOrientBench.png">   | 2024-10 |             [Github](https://github.com/jhCOR/EgoOrientBench)          | 
| [![Publish](https://img.shields.io/badge/Conference-ACL'25-blue)]()<br/>[ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models](https://arxiv.org/pdf/2410.04659) |   <img width="700" alt="image" src="imgs/ActiView.png">   | 2024-10 |         [Github](https://wangphoebe.github.io/actiview_homepage/)         |
|  [![Publish](https://img.shields.io/badge/Conference-ICLR'25-blue)]()<br/>[ DOES SPATIAL COGNITION EMERGE IN FRONTIER MODELS?](https://arxiv.org/pdf/2410.06468) |   <img width="700" alt="image" src="imgs/SPACE.png">   | 2024-10 | -
|  [![Publish](https://img.shields.io/badge/Conference-ICLR'25-blue)]()<br/>[Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities](https://arxiv.org/abs/2410.17385) |   <img width="700" alt="image" src="imgs/Comfort.png">   | 2024-10 |             [Github](https://github.com/sled-group/COMFORT)          | 
|  <br/>[R2D3: Imparting Spatial Reasoning by Reconstructing 3D Scenes from 2D Images](https://openreview.net/pdf?id=Ku4lylDpjq) |   <img width="700" alt="image" src="imgs/R2D3.png">   | 2024-10 |             [Github](https://github.com/arijitray1993/r2d3/)          | 
|  [![Publish](https://img.shields.io/badge/Conference-EMNLP'24-blue)]()<br/>[Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2409.09788) |   <img width="700" alt="image" src="imgs/Q.png">   | 2024-09 |             [Github](https://github.com/andrewliao11/Q-Spatial-Bench-code)          |          -     |
|  [![Publish](https://img.shields.io/badge/Conference-NIPSW'24-blue)]()<br/>[Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?](https://arxiv.org/abs/2409.17080) |   <img width="700" alt="image" src="imgs/SVAT.png">   | 2024-09 |       -     |
|  <br/>[VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs](https://arxiv.org/abs/2407.01863) |   <img width="700" alt="image" src="imgs/VSP.png">   | 2024-07 |             [Github](https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning)    |
|  [![Publish](https://img.shields.io/badge/Conference-ACL'24-blue)]()<br/>[EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models](https://arxiv.org/abs/2406.05756) |   <img width="700" alt="image" src="imgs/Emb.png">   | 2024-06 |             [Github](https://github.com/mengfeidu/EmbSpatial-Bench)          |
|  [![Publish](https://img.shields.io/badge/Conference-EMNLP'24-blue)]()<br/>[TopViewRS: Vision-Language Models as Top-View Spatial Reasoners](https://arxiv.org/abs/2406.05756) |   <img width="700" alt="image" src="imgs/Topviewers.png">   | 2024-06 |             [Github](https://github.com/cambridgeltl/topviewrs)          |
|  [![Publish](https://img.shields.io/badge/Conference-NIPS'24-blue)]()<br/>[Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models](https://arxiv.org/abs/2406.14852) |   <img width="700" alt="image" src="imgs/SpatialEval.png">   | 2024-06 |      [Github](https://github.com/jiayuww/SpatialEval)      |     -  |
|  [![Publish](https://img.shields.io/badge/Conference-IJCAI'24-blue)]()<br/>[GSR-Bench: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs](https://arxiv.org/abs/2406.13246) |   <img width="700" alt="image" src="imgs/GSR.png">   | 2024-06 |             -      |
|  [![Publish](https://img.shields.io/badge/Conference-IJCAI'24-blue)]()<br/>[Reframing Spatial Reasoning Evaluation in Language Models: A Real-World Simulation Benchmark for Qualitative Reasoning](https://arxiv.org/abs/2405.15064) |   <img width="700" alt="image" src="imgs/RoomSpace.png">   | 2024-05 |             [Github](https://github.com/Fangjun-Li/RoomSpace)          |
|   [![Publish](https://img.shields.io/badge/Conference-ACL'22-blue)]()[![Star](https://img.shields.io/github/stars/xxxiaol/spatial-commonsense.svg?style=social&label=Star)]()<br/>[Things not Written in Text: Exploring Spatial Commonsense from Visual Signals](https://arxiv.org/abs/2203.08075) |   <img width="700" alt="image" src="imgs/common.png">   | 2022-03 |   [Github](https://github.com/xxxiaol/spatial-commonsense)
|  [![Publish](https://img.shields.io/badge/Conference-CVPR'20-blue)]()<br/>[SPARE3D: ADataset for SPAtial REasoning on Three-View Line Drawings](https://arxiv.org/abs/2003.14034) |   <img width="700" alt="image" src="imgs/SPARE3D.jpg">   | 2020-03 |             [Github](https://github.com/ai4ce/spare3d)          |
### Text based
| Title                                                        |                         Introduction                         |    Date    |                           Code                           |
| :----------------------------------------------------------- | :----------------------------------------------------------: | :--------: | :------------------------------------------------------: |
|  <br/>[Do Multimodal Language Models Really Understand Direction? A Benchmark for Compass Direction Reasoning](https://arxiv.org/abs/2412.16599) |   <img width="700" alt="image" src="imgs/CDR.png">   | 2024-12 |      -
|  <br/>[GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial Reasoning](https://arxiv.org/abs/2407.01892) |   <img width="700" alt="image" src="imgs/grasp.png">   | 2024-07 |  -   |

## Findings
| Title                                                        |                         Introduction                         |    Date    |                           Code                           |
| :----------------------------------------------------------- | :----------------------------------------------------------: | :--------: | :------------------------------------------------------: |
|  <br/>[Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks](https://arxiv.org/pdf/2510.25760) |   <img width="700" alt="image" src="imgs/MMSR.png">   | 2025-10 |    [Github](https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning)   |
|  <br/>[How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905) |   <img width="700" alt="image" src="imgs/howfar.png">   | 2025-09 |     [Github](https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/)   |
|  <br/>[Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/pdf/2508.13142v1) |   <img width="700" alt="image" src="imgs/GPT5.png">   | 2025-08 |    -  |
|  <br/>[A Call for New Recipes to Enhance Spatial Reasoning in MLLMs](https://arxiv.org/pdf/2504.15037) |   <img width="700" alt="image" src="imgs/call.png">   | 2025-03 |     [Github](https://aka.ms/GeneralAI)   |
|  <br/>[Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas](https://arxiv.org/abs/2503.01773) |   <img width="700" alt="image" src="imgs/AdaptVIS.png">   | 2025-03 |     [Github](https://github.com/shiqichen17/AdaptVis)   |
|  <br/>[Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models](https://arxiv.org/abs/2503.17349) |   <img width="700" alt="image" src="imgs/spatialaware.png">   | 2025-03 |     [Github](https://user074.github.io/respatialaware/)   |

## Applications
| Title                                                        |                         Introduction                         |    Date    |                           Code                           |
| :----------------------------------------------------------- | :----------------------------------------------------------: | :--------: | :------------------------------------------------------: |
|  [![Publish](https://img.shields.io/badge/Conference-CVPR'25-green)]()<br/>[Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection](https://arxiv.org/pdf/2412.04455) | <img width="700" alt="image" src="imgs/CAM.png"> | 2024-12 | [Github](https://zhoues.github.io/Code-as-Monitor/) |
|  <br/>[SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models](https://arxiv.org/pdf/2505.04911) |   <img width="700" alt="image" src="imgs/SpatialPrompting.png">   | 2025-05 |   - |
|  <br/>[InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning](https://arxiv.org/pdf/2505.13888) |   <img width="700" alt="image" src="imgs/InSpire.png">   | 2025-05 |     [Github](https://koorye.github.io/proj/Inspire) |
|  <br/>[EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks](https://arxiv.org/pdf/2503.11089) |   <img width="700" alt="image" src="imgs/EmbodiedVSR.png">   | 2025-03 |   - |
|  <br/>[SOFAR: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation](https://arxiv.org/pdf/2502.13143) |   <img width="700" alt="image" src="imgs/SoFar.png">   | 2025-02 |     [Github](https://qizekun.github.io/sofar) |
|  <br/>[ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning](https://arxiv.org/abs/2503.23297) |   <img width="700" alt="image" src="imgs/ReasonGrounder.png">   | 2025-03 |     [Github](https://zhenyangliu.github.io/ReasonGrounder/) |
|  <br/>[VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning](https://arxiv.org/abs/2502.0093) |   <img width="700" alt="image" src="imgs/VL-Nav.png">   | 2025-02 |     -  |
|  <br/>[SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Models](https://arxiv.org/abs/2501.15830) |   <img width="700" alt="image" src="imgs/SpatialVLA.png">   | 2025-01 |      [Github](https://github.com/SpatialVLA/SpatialVLA)  |
|  <br/>[EMMA-X:AnEmbodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning](https://arxiv.org/abs/2412.11974) |   <img width="700" alt="image" src="imgs/EmmaX.png">   | 2024-12 |      [Github](https://github.com/declare-lab/Emma-X)  |
|  [![Publish](https://img.shields.io/badge/Conference-CVPR'24-blue)]()<br/>[Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning](https://arxiv.org/abs/2404.03658) |   <img width="700" alt="image" src="imgs/KYN.png">   | 2024-04 |     [Github](https://github.com/ruili3/Know-Your-Neighbors)     |
|  [![Publish](https://img.shields.io/badge/Conference-WACV'24-blue)]()<br/>[Improving Vision-and-Language Reasoning via Spatial Relations Modeling](https://arxiv.org/abs/2311.05298) |   <img width="700" alt="image" src="imgs/OPR.png">   | 2023-11 |    -     |
